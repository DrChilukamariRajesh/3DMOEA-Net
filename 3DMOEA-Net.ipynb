{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c7d91a9-b49c-4856-9c3c-0c9dbebc8bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bbe13a6-3c78-49d0-afc5-9ec3e3525d20",
   "metadata": {},
   "source": [
    "# Importing all necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5afee3f1-8ef3-4e90-aa78-f7dda88aed0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "import shutil\n",
    "import tempfile\n",
    "import numpy as np\n",
    "import pandas, csv\n",
    "from glob import glob\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from torchsummary import summary\n",
    "from monai.losses import DiceLoss\n",
    "from monai.utils import set_determinism\n",
    "from monai.handlers.utils import from_engine\n",
    "from monai.utils.enums import MetricReduction\n",
    "from monai.transforms import Activations, AsDiscrete, Compose\n",
    "from monai.data import CacheDataset, DataLoader, decollate_batch\n",
    "from monai.metrics import DiceMetric, HausdorffDistanceMetric, MeanIoU\n",
    "\n",
    "from moed.ega import *\n",
    "from moed.utils import *\n",
    "from moed.brats2021 import *\n",
    "from moed.searchspace import *\n",
    "from moed.moed3d import MOED3D\n",
    "from moed.task01_braintumour import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89133c8a-5be3-46fb-845e-6a8d365f00b5",
   "metadata": {},
   "source": [
    "# Dataset Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe330e2-773f-42d8-93ba-a2af13e6f206",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_determinism(seed=0)\n",
    "task_name =\"Task01_BrainTumour\" \n",
    "root_dir = \"files/\"+ task_name\n",
    "data_dir = \"Datasets/\"+ task_name\n",
    "dataset_name = task_name.split(\"_\")[1]\n",
    "json_filename = data_dir+\"/\"+task_name+\".json\"\n",
    "file_name_res = f\"results/{dataset_name}_single_res.csv\"\n",
    "print(file_name_res)\n",
    "create_dir(root_dir)\n",
    "create_dir('results/')\n",
    "create_res_file(file_name_res)\n",
    "\n",
    "\n",
    "# Cache rate\n",
    "cr = 0.1\n",
    "batchsize = 4\n",
    "# number of workers\n",
    "now = batchsize * 2\n",
    "# Training epochs\n",
    "max_epochs = 80\n",
    "# Test epochs\n",
    "n_epoch = 60\n",
    "img_size = 96\n",
    "val_interval = 2\n",
    "img_shape = (img_size, img_size, img_size)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "post_trans = Compose([Activations(sigmoid=True), AsDiscrete(threshold=0.5)])\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "(train_images, train_labels), (val_images, val_labels), (test_images, test_labels) = datafold_read(data_dir, json_filename, 0)\n",
    "train_files = [{\"image\": image_name, \"label\": label_name} for image_name, label_name in zip(train_images, train_labels)]\n",
    "val_files = [{\"image\": image_name, \"label\": label_name} for image_name, label_name in zip(val_images, val_labels)]\n",
    "test_files = [{\"image\": image_name, \"label\": label_name} for image_name, label_name in zip(test_images, test_labels)]\n",
    "print(len(train_files), len(val_files), len(test_files))\n",
    "\n",
    "train_transform = transform_train(img_size)\n",
    "train_ds = CacheDataset(data=train_files, transform=train_transform, cache_rate=cr, num_workers=now)\n",
    "train_loader = DataLoader(train_ds, batch_size=batchsize, shuffle=True, num_workers=now)\n",
    "\n",
    "val_transform = transform_val(img_shape)\n",
    "val_ds = CacheDataset(data=val_files, transform=val_transform, cache_rate=cr, num_workers=now)\n",
    "val_loader = DataLoader(val_ds, batch_size=batchsize, shuffle=False, num_workers=now)\n",
    "\n",
    "## Testing\n",
    "test_org_transforms = transform_test_org(img_shape)\n",
    "test_org_ds = CacheDataset(data=test_files, transform=test_org_transforms, cache_rate=cr, num_workers=now)\n",
    "test_org_loader = DataLoader(test_org_ds, batch_size=batchsize, shuffle=False, num_workers=now)\n",
    "post_transforms = transform_post(test_org_transforms)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4264b51d-3f8d-4a8c-b290-1109ebcb0795",
   "metadata": {},
   "source": [
    "# Visualize the image with corresponding label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4585bfb4-ba20-4e5f-bda9-d4e4e59de61e",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    for tod in test_org_ds:\n",
    "        plt.figure(\"image\", (24, 6))\n",
    "        for i in range(4):\n",
    "            plt.subplot(1, 4, i + 1)\n",
    "            plt.title(f\"image channel {i}\")\n",
    "            plt.imshow(tod[\"image\"][i, :, :, 60].detach().cpu(), cmap=\"gray\")        \n",
    "        plt.figure(\"label\", (18, 6))\n",
    "        for i in range(3):\n",
    "            plt.subplot(1, 3, i + 1)\n",
    "            plt.title(f\"label channel {i}\")\n",
    "            plt.imshow(tod[\"label\"][i, :, :, 55].detach().cpu())\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a663bdd-f454-4b03-9549-78c40e138e0d",
   "metadata": {},
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3957e0c7-28a6-43d2-a34f-ae7010aedcac",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def runModel(model_name, model, optimizer, loss_function, ptcl, file_name_res):     \n",
    "    exists, dsl, nop, fps = checkCache([file_name_res], ptcl)\n",
    "    if exists:\n",
    "        return dsl, nop, fps\n",
    "    \n",
    "    model = model.to(device)\n",
    "    Total_params, flops, fps = calc_pff(model, img_size, 4,4)\n",
    "    print(f\"No.of params - {Total_params}\")\n",
    "   \n",
    "    lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=max_epochs)\n",
    "    checkpoint_name = model_name+\"_\"+dataset_name+\"_best_metric.pth\"\n",
    "    print(checkpoint_name)\n",
    "    \n",
    "    dice_func = DiceMetric(include_background=True, reduction=MetricReduction.MEAN_BATCH, get_not_nans=True)    \n",
    "    hdm_func = HausdorffDistanceMetric(include_background=True, percentile=95, reduction=MetricReduction.MEAN_BATCH)\n",
    "  \n",
    "    run_acc = AverageMeter()\n",
    "    run_hdm = AverageMeter()\n",
    "\n",
    "    best_metric = -1\n",
    "    best_metric_epoch = -1\n",
    "    best_metrics_epochs_and_time = [[], [], []]\n",
    "    epoch_loss_values = []\n",
    "\n",
    "    total_start = time.time()\n",
    "    \n",
    "    ## Training starts here\n",
    "    for epoch in range(max_epochs):\n",
    "        epoch_start = time.time()\n",
    "        print(\"-\" * 10)\n",
    "        print(f\"epoch {epoch + 1}/{max_epochs}\")\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        step = 0\n",
    "        for batch_data in train_loader:\n",
    "            step += 1\n",
    "            inputs, labels = (\n",
    "                batch_data[\"image\"].to(device),\n",
    "                batch_data[\"label\"].to(device),\n",
    "            )\n",
    "            optimizer.zero_grad()\n",
    "            with torch.cuda.amp.autocast():\n",
    "                outputs = model(inputs)\n",
    "                loss = loss_function(outputs, labels)\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            epoch_loss += loss.item()\n",
    "        lr_scheduler.step()\n",
    "        epoch_loss /= step\n",
    "        epoch_loss_values.append(epoch_loss)\n",
    "        print(f\"epoch {epoch + 1} average loss: {epoch_loss:.4f}\")\n",
    "        \n",
    "        ## Validation\n",
    "        if epoch >= n_epoch:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                for val_data in val_loader:\n",
    "                    val_inputs, val_labels = (\n",
    "                        val_data[\"image\"].to(device),\n",
    "                        val_data[\"label\"].to(device),\n",
    "                    )\n",
    "                    val_outputs = inference(val_inputs, model)\n",
    "                    val_outputs = [post_trans(i) for i in decollate_batch(val_outputs)]\n",
    "                \n",
    "                    #dice score calculation \n",
    "                    dice_func(y_pred=val_outputs, y=val_labels)\n",
    "                    acc, not_nans = dice_func.aggregate()\n",
    "                    # print(acc.cpu().numpy())\n",
    "                    run_acc.update(acc.cpu().numpy())\n",
    "                    dice_func.reset()\n",
    "                    \n",
    "                    #hdm calculation\n",
    "                    hdm_func(y_pred=val_outputs, y=val_labels)\n",
    "                    hd = hdm_func.aggregate()\n",
    "                    # print(hd.cpu().numpy())\n",
    "                    run_hdm.update(hd.cpu().numpy()) \n",
    "                    hdm_func.reset()  \n",
    "                    \n",
    "                print(f\"current epoch: {epoch + 1}, Dice_Avg: {np.mean(run_acc.val)}, dice_tc: {run_acc.val[0]}, dice_wt: {run_acc.val[1]},\\\n",
    "                        dice_et: {run_acc.val[2]}, HDM_Avg: {np.mean(run_hdm.val)}, hdm_tc: {run_hdm.val[0]}, hdm_wt: {run_hdm.val[1]}, hdm_et: {run_hdm.val[2]}\")\n",
    "                \n",
    "                if np.mean(run_acc.val) > best_metric:\n",
    "                    best_metric = np.mean(run_acc.val)\n",
    "                    best_metric_epoch = epoch + 1\n",
    "                    best_metrics_epochs_and_time[0].append(best_metric)\n",
    "                    best_metrics_epochs_and_time[1].append(best_metric_epoch)\n",
    "                    best_metrics_epochs_and_time[2].append(time.time() - total_start)\n",
    "                    torch.save(model.state_dict(), os.path.join(root_dir, checkpoint_name),)\n",
    "                    print(f\"saved new best metric model at {best_metric_epoch}\")\n",
    "                              \n",
    "        \n",
    "        print(f\"time consuming of epoch {epoch + 1} is: {(time.time() - epoch_start):.4f}\")\n",
    "        if epoch > 20 and best_metric < 0.15 and np.mean(run_acc.val) < 0.15:\n",
    "            print(f\"Stopping training after {epoch}th epoch as {np.mean(run_acc.val)} dice not increasing\")\n",
    "            break\n",
    "    total_end = time.time()\n",
    "    total_time = ((total_end - total_start)/60)/60\n",
    "\n",
    "    print(f\"Train completed, best_metric: {best_metric:.4f} at epoch: {best_metric_epoch},\\\n",
    "                total time: {total_time}.\")\n",
    "\n",
    "    \n",
    "    ## Testing\n",
    "    model.load_state_dict(torch.load(os.path.join(root_dir, checkpoint_name)))\n",
    "    model.eval()\n",
    "\n",
    "    dice_func.reset()\n",
    "    hdm_func.reset()\n",
    "    run_acc.reset()\n",
    "    run_hdm.reset()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for test_data in test_org_loader:\n",
    "            test_inputs, test_labels = (\n",
    "                test_data[\"image\"].to(device),\n",
    "                test_data[\"label\"].to(device),\n",
    "            )\n",
    "            test_outputs = inference(test_inputs, model)\n",
    "            test_outputs = [post_trans(i) for i in decollate_batch(test_outputs)]\n",
    "            \n",
    "            #dice score calculation\n",
    "            dice_func(y_pred=test_outputs, y=test_labels)\n",
    "            acc, not_nans = dice_func.aggregate()\n",
    "            run_acc.update(acc.cpu().numpy())\n",
    "            dice_func.reset()\n",
    "\n",
    "            #hdm calculation\n",
    "            hdm_func(y_pred=test_outputs, y=test_labels)\n",
    "            hd = hdm_func.aggregate()\n",
    "            run_hdm.update(hd.cpu().numpy())\n",
    "            hdm_func.reset()\n",
    "\n",
    "        print(f\"Final validation stats {epoch}/{max_epochs - 1}, Dice_Avg: {np.mean(run_acc.val)}, \\\n",
    "            dice_tc: {run_acc.val[0]}, dice_wt: {run_acc.val[1]}, dice_et: {run_acc.val[2]},  \\\n",
    "            HDM_Avg: {np.mean(run_hdm.val)}, hdm_tc: {run_hdm.val[0]}, hdm_wt: {run_hdm.val[1]}, hdm_et: {run_hdm.val[2]}, \\\n",
    "            time {time.time() - total_end}s \")  \n",
    "                  \n",
    "    l=[]\n",
    "    l.extend([model_name, best_metric, Total_params, flops, fps, total_time, np.mean(run_acc.val), run_acc.val[0], run_acc.val[1], run_acc.val[2], \\\n",
    "              np.mean(run_hdm.val),  run_hdm.val[0], run_hdm.val[1], run_hdm.val[2], ptcl, convert_Time(total_start), convert_Time(total_end), best_metric_epoch, task_name])     \n",
    "    saveValues(file_name_res,l)\n",
    "\n",
    "    return 1-np.mean(run_acc.val), Total_params/1e6, flops"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86dfd155-64f5-4c57-9656-4881edc483a4",
   "metadata": {},
   "source": [
    "# EGA- NAS Evolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5cdb06b-97cb-4f1a-ab1e-3f20aad6a7dc",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "np.random.seed(4345)\n",
    "\n",
    "# individual length\n",
    "ch_len = 56    \n",
    "\n",
    "# Number of generations\n",
    "max_gen = 20   \n",
    "\n",
    "# threshold\n",
    "z = 2\n",
    "\n",
    "# Population size\n",
    "pop_size = 20\n",
    "\n",
    "# crossover rate\n",
    "r_cross = 0.9\n",
    "\n",
    "# mutation rate\n",
    "r_mut = 1.0 / float(ch_len)\n",
    "\n",
    "# Population evaluation\n",
    "def evaluation(t, pop):\n",
    "    for i, ind in enumerate(pop):\n",
    "        print(\"----\\n\\n\\n{} {} {}\".format(t, i, ind))\n",
    "        print(ind, type(ind))\n",
    "        model, optimizer, loss_function, ptcl = encoding_ch(ind, file_name_res)\n",
    "        # try:\n",
    "        objective_values[i] = runModel(f\"{t}_{i}\", model, optimizer, loss_function, ptcl, file_name_res)\n",
    "        # except Exception as e:\n",
    "        #     print(e)\n",
    "        #     objective_values[i] = 99, 9999999999, 99999999999\n",
    "            \n",
    "        print(objective_values[i])\n",
    "    return objective_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b321b849-8bc6-402e-adb9-b8ba2f2f9f19",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Population and Objective values Inititalization\n",
    "pop, objective_values = init_pop(pop_size, ch_len), init_obj(pop_size)\n",
    "\n",
    "# Initial population evaluation\n",
    "objective_values = evaluation(0, pop)\n",
    "\n",
    "# Enumerate generations\n",
    "for gen in range(1, max_gen):\n",
    "    print(f\"\\n\\n\\n{gen} Generation:\")\n",
    "    \n",
    "    # E-GA Operations\n",
    "    selected = np.asarray([parent_selection(pop, objective_values, z) for _ in range(pop_size)])\n",
    "    print(selected)\n",
    "    \n",
    "    child = list()\n",
    "    for i in range(0, pop_size, 2):\n",
    "        # get selected parents in pairs\n",
    "        p1, p2 = selected[i], selected[i+1]\n",
    "        # crossover\n",
    "        for c in crossover(p1, p2, r_cross):\n",
    "            # mutation\n",
    "            c = mutation(c, r_mut)\n",
    "            # store for next generation\n",
    "            child.append(c)\n",
    "    child = np.asarray(child)\n",
    "           \n",
    "    # evaluate all candidates in the child population\n",
    "    child_objective_values = evaluation(gen, child)\n",
    "    \n",
    "    # Choose new pop for next generation\n",
    "    pop, objective_values = selection(np.concatenate([pop, child]), np.concatenate([objective_values, child_objective_values]), pop_size)\n",
    "\n",
    "print(pop, objective_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7dc7706-f0cf-4e72-9071-d81d4adf80f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
